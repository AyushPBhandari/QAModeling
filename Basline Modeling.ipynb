{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import nltk\n",
    "from InferSent.models import InferSent\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_infersent(V=2):\n",
    "    '''\n",
    "    Builds the infersent model using either GloVe or fastText\n",
    "    '''\n",
    "    MODEL_PATH = 'encoder/infersent%s.pkl' %V\n",
    "    if V == 2:\n",
    "        W2V_PATH = 'fastText/crawl-300d-2M.vec'\n",
    "    elif V == 1:\n",
    "        W2V_PATH = 'GloVe/glove.840B.300d.txt'\n",
    "    \n",
    "    params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048, \\\n",
    "                    'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
    "    infersent = InferSent(params_model)\n",
    "    infersent.load_state_dict(torch.load(MODEL_PATH))\n",
    "    infersent.set_w2v_path(W2V_PATH)\n",
    "\n",
    "    return infersent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(loc: str):\n",
    "    '''\n",
    "    Get the dataset from file location 'loc'\n",
    "    '''\n",
    "    with open(loc) as infile:\n",
    "        dataset = json.load(infile)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(infersent, sentences: list):\n",
    "    '''\n",
    "    Use sentences to build a sentence embedding for each context using infersent.\n",
    "    Returns a list of sentence embeddings\n",
    "    '''\n",
    "\n",
    "    print(\"Getting Sentence Embeddings for %d sentences\", len(sentences))\n",
    "    # outputs a numpy array with n vectors of dimension 4096\n",
    "    context_embeddings = []\n",
    "    for sentence in sentences:\n",
    "        # sentence is actually a list of sentences for context_i\n",
    "        embeddings = infersent.encode(sentence, tokenize=True)\n",
    "        context_embeddings.append(embeddings)\n",
    "    \n",
    "    return np.asarray(context_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextWords = [[context[i].split()] for i in range(len(context))]\n",
    "questionWords = [[questions[i].split()] for i in range(len(questions))]\n",
    "answerWords = []\n",
    "for i in range(len(answers)):\n",
    "    if len(answers[i]) > 0:\n",
    "        current = answers[i][0].split()\n",
    "    else:\n",
    "        current = \"\"\n",
    "    answerWords.append(current)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_data(dataset: dict):\n",
    "    '''\n",
    "    Retrieves context, questions, and targets from the data\n",
    "    Context will return a list of lists for each sentence in a given context\n",
    "    Questions will return a list of lists of questions for each context\n",
    "    Targets will return a list of target values that correspond to each question.\n",
    "    Target values are equivalent to the sentence number within the context that contains the answer to the question\n",
    "    '''\n",
    "    data = dataset['data']\n",
    "    target = []\n",
    "    ctx = [] \n",
    "    questions = [] \n",
    "    answers = []\n",
    "    for topic in data:\n",
    "        sentences = []\n",
    "        for paragraph in topic['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            cont_sents = nltk.sent_tokenize(context)\n",
    "            ctx.append(cont_sents)\n",
    "\n",
    "            c_question = []\n",
    "            c_answer = []\n",
    "            for qas in paragraph['qas']:\n",
    "                if qas['is_impossible']:\n",
    "                    # skip impossible questions\n",
    "                    continue\n",
    "                question = qas['question']\n",
    "                answer = qas['answers'][0]['text']\n",
    "                c_question.append(question)\n",
    "                c_answer.append(answer)\n",
    "\n",
    "                ans_pos = qas['answers'][0]['answer_start']\n",
    "\n",
    "                acc = 0\n",
    "                # find which sentence the answer is part of\n",
    "                for i, sent in enumerate(cont_sents):\n",
    "                    acc += len(sent)\n",
    "                    if acc > ans_pos:\n",
    "                        # answer is in sentence i\n",
    "                        target.append(i)\n",
    "                        break\n",
    "            \n",
    "            questions.append(c_question)\n",
    "            answers.append(c_answer)\n",
    "    \n",
    "    return ctx, questions, answers, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(infersent, context: list):\n",
    "    '''\n",
    "    Flattens the context and then builds the vocab\n",
    "    '''\n",
    "    flat_context = [sentence for c in context for sentence in c] \n",
    "    infersent.build_vocab(flat_context, tokenize=True)\n",
    "\n",
    "    return infersent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(a,b):\n",
    "    '''\n",
    "    Calculate the cosine similiarity between a and b\n",
    "    cos_sim = a.b / |a||b|\n",
    "    '''\n",
    "    return np.dot(a,b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load inferscent pre-trained model\n",
    "infersent = get_infersent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load dataset\n",
    "dataset = get_dataset(\"train-v2.0.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parse the dataset\n",
    "context, questions, answers, target = retrieve_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 89670(/110274) words with w2v vectors\n",
      "Vocab size : 89670\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "InferSent(\n",
       "  (enc_lstm): LSTM(300, 2048, bidirectional=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_vocab(infersent, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Sentence Embeddings for %d sentences 22\n",
      "Getting Sentence Embeddings for %d sentences 22\n"
     ]
    }
   ],
   "source": [
    "ctx_embed = get_embedding(infersent, context[:22])\n",
    "q_embed = get_embedding(infersent, questions[:22])\n",
    "# a_embed = get_embedding(infersent, answers[:22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "## finding max sentences in any paragraph\n",
    "\n",
    "max = 0\n",
    "for i in range(len(ctx_embed)):\n",
    "    temp = len(ctx_embed[i])\n",
    "    if temp > max:\n",
    "        max = temp\n",
    "print(max)\n",
    "\n",
    "n = len(ctx_embed)\n",
    "feature_vectors = []\n",
    "for i in range(n):\n",
    "    quests = q_embed[i]\n",
    "    cntxs = ctx_embed[i]\n",
    "    for j in range(len(quests)):\n",
    "        similarities = []\n",
    "        for k in range(len(cntxs)):\n",
    "            a = cos_similarity(quests[j], cntxs[k])\n",
    "            similarities.append(a)\n",
    "        if len(similarities) < max:\n",
    "            diff = max - len(similarities)\n",
    "            for i in range(diff):\n",
    "                similarities.append(1.0)\n",
    "        feature_vectors.append(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39177325, 0.2917924, 0.33045757, 0.3469124, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(feature_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
